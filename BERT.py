# -*- coding: utf-8 -*-
"""b2b_60.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hwqGA9IHdrM9pICpCpstxjQdVxcheOl

## **Dependency**
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers==4.12.0
# !pip install datasets evaluate transformers[sentencepiece]
# !pip install accelerate
# !pip install numpy
# #!pip install cloud-tpu-client==0.10 torch==1.11.0
# !apt install git-lfs
# !pip install datasets evaluate transformers rouge-score nltk
# !pip install -q peft transformers datasets
# !pip install --upgrade tensorflow==2.15.*
# !pip install --upgrade datasets pyarrow
# !pip install -q bitsandbytes datasets accelerate loralib
# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
# !pip install tensorboard tensorboardX
# !pip install -U tensorboard-plugin-profile
# !pip install bert-score

# checking the GPU and CUDA
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

# checking the RAM
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# Commented out IPython magic to ensure Python compatibility.
# download file from drive，and only use it when you need lora from scratch.py.
from google.colab import drive
drive.mount('/content/drive')
# %cd "/content/drive/My Drive/Colab"
# %ls

import transformers
print(transformers.__version__)

from transformers.utils import send_example_telemetry
send_example_telemetry("summarization_notebook", framework="pytorch")

#parallel computing
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# storing the authentication token from the Hugging Face.
from huggingface_hub import notebook_login
notebook_login()

"""### **Data Preprocessing**

"""

from transformers import BertTokenizer
import datasets

raw_datasets = datasets.load_dataset("jhan21/amazon-food-reviews-dataset", split="train")

from transformers import EncoderDecoderModel, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "google-bert/bert-base-uncased", "google-bert/bert-base-uncased"
)  # initialize Bert2Bert from pre-trained checkpoints

# training
model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size
model.save_pretrained("bert2bert")

from sklearn.model_selection import train_test_split
from datasets import Dataset

x = raw_datasets['Text']
y = raw_datasets['Summary']

raw_datasets = Dataset.from_dict({"Text": x, "Summary": y})

# remove the none data
index_to_remove = [i for i, (text, summary) in enumerate(zip(x, y)) if not text or not summary]

filtered_x = [text for i, text in enumerate(x) if i not in index_to_remove]
filtered_y = [summary for i, summary in enumerate(y) if i not in index_to_remove]

# split the dataset
x_train, x_temp, y_train, y_temp = train_test_split(filtered_x, filtered_y, test_size=0.2, random_state=42)
x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.1, random_state=42)

def dict(x,y):
  set={'Text': x, 'Summary': y}
  set=Dataset.from_dict(set)
  return set


traindata = dict(x_train,y_train)
validationdata =dict(x_valid,y_valid)
testdata = dict(x_test, y_test)

# training the model on  a small subset
import pandas as pd

def split(set,ratio):
  cut_point=int(len(set)*ratio)
  small_set=Dataset.from_dict(set[0:cut_point])
  return small_set

ratio=0.6
train = split(traindata,ratio)
validation= split(validationdata,ratio)
test=split(testdata,0.05)

print(train)
print(validation)
print(test)

# prompt
def start_prompt(num,text):
  if num==1:
    prompt=f'Summarize the following text with a maximum of {len(tokenizer.tokenize(text))} words:'
  if num==2:
    sentences = [sentence.strip() for sentence in text.split(".") if sentence.strip()]
    prompt=f'Can you succinctly summarize in {len(sentences)} sentences the following passage?'
  if num==3:
    prompt='Could you provide me with a brief summary of the following text?'
  if num==4:
    prompt='summarize:'
  if num==5:
    prompt='TL;DR:'
  return prompt

import random
encoder_max_length=512
decoder_max_length=64

def process_data_to_model_inputs(batch):
  # tokenize the inputs and labels
  prompt = [start_prompt(random.randint(1, 5), sum) + doc for doc, sum in zip(batch["Text"], batch["Summary"])]
  inputs = tokenizer(prompt, padding="max_length", truncation=True, max_length=encoder_max_length,return_tensors="pt").to(device)
  outputs = tokenizer(batch["Summary"], padding="max_length", truncation=True, max_length=decoder_max_length,return_tensors="pt").to(device)

  batch["input_ids"] = inputs.input_ids
  batch["attention_mask"] = inputs.attention_mask
  batch["decoder_input_ids"] = outputs.input_ids
  batch["decoder_attention_mask"] = outputs.attention_mask
  batch["labels"] = outputs.input_ids.clone()

  # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.
  # We have to make sure that the PAD token is ignored
  batch["labels"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch["labels"]]
  return batch


train_data = train.map(
    process_data_to_model_inputs,
    batched=True,
    remove_columns=['Text', 'Summary']
)
train_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],
)


val_data = validation.map(
    process_data_to_model_inputs,
    batched=True,
    remove_columns=['Text', 'Summary']
)
val_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids", "decoder_attention_mask", "labels"],
)

"""### **Warm-starting the Encoder-Decoder Model**"""

# save and load from pretrained
bert2bert = EncoderDecoderModel.from_pretrained("bert2bert")
original_model=bert2bert
bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size
bert2bert.config.max_length = 142
bert2bert.config.min_length = 56
bert2bert.config.no_repeat_ngram_size = 3
bert2bert.config.early_stopping = True
bert2bert.config.length_penalty = 2.0
bert2bert.config.num_beams = 4

bert2bert.to(device)

bert2bert.device

def trainable_model_params(model):
  trainable_params=0
  all_params=0
  for _,param in model.named_parameters():
    all_params+=param.numel()
    if param.requires_grad:
      trainable_params+=param.numel()
  return f"trainable params: {trainable_params}\nall params: {all_params}\npercentage of trainable params: {100*trainable_params/all_params}"

trainable_model_params(bert2bert)

"""

## **Setup the LoRA model for Fine-Tuning**

"""

# set up the configuration
from peft import LoraConfig, get_peft_model, TaskType

# Define LoRA Config
lora_config = LoraConfig(
    r=16, # Rank
    lora_alpha=32,
    target_modules=["query", "value"],
    lora_dropout=0.05,
    bias="none",
    fan_in_fan_out=False,
    task_type="SEQ_2_SEQ_LM",
)

# add LoRA adaptor layers/parameters to the original LLM to be trained.
lora_model = get_peft_model(bert2bert,lora_config)

trainable_model_params(lora_model)

"""### **Fine-Tuning Warm-Started Encoder-Decoder Models**"""

from transformers import GenerationConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq
import time

output_dir = f'/bert-lora-ar-training-1'

# Define training args
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    auto_find_batch_size=True,
    num_train_epochs=1,
    logging_strategy="steps",
    logging_steps=500,
    gradient_accumulation_steps=2,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    weight_decay=0.01,
    push_to_hub=True,
    fp16=True,
    predict_with_generate=True,
    report_to=["tensorboard"],
)

# data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model)

# create a trainer
peft_trainer = Seq2SeqTrainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

"""# **Training**"""

# Commented out IPython magic to ensure Python compatibility.
# use TensorBoard to monitor the training process
# %tensorboard --logdir '{output_dir}'/runs

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# train model
peft_trainer.train()

peft_trainer.evaluate()

# Save our LoRA model & tokenizer results in local drive.
peft_model_path="./b2b-lora-60"
peft_trainer.model.save_pretrained(peft_model_path)
peft_trainer.model.push_to_hub('b2b-lora-60')
tokenizer.save_pretrained(peft_model_path)

from peft import PeftModel, PeftConfig
peft_model = PeftModel.from_pretrained(bert2bert,
                                       "./b2b-lora-60",
                                       torch_dtype=torch.bfloat16,
                                       is_trainable=False)

# push to Hub
peft_model.push_to_hub("b2b-lora-ar-60")

import shutil

# 模型目录在 Colab 中的路径
colab_model_dir = '/content/b2b-lora-60'
# 模型目录要保存到 Google Drive 中的路径
drive_model_dir = '/content/drive/My Drive/b2b-lora-60'

# 将整个目录复制到 Google Drive
shutil.copytree(colab_model_dir, drive_model_dir)

print("Model directory copied to Google Drive.")

peft_model

peft_trainer.get_learning_rates()

peft_trainer.get_train_dataloader()

test=split(testdata,0.05)

"""# **Evaluate**

**human evaluation**
"""

#basic inference
from evaluate import evaluator
from transformers import AutoModelForSeq2SeqLM, pipeline
def example(text):
  print(text)
  input_prompt= f"Summarize:{text}"
  input_ids = tokenizer(input_prompt, padding="max_length", truncation=True, max_length=512, return_tensors="pt").input_ids.to(device)
  original_model_outputs = original_model.generate(input_ids=input_ids, max_length=64)
  original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)
  #print(original_model_text_output)
  peft_model_outputs = peft_model.generate(input_ids=input_ids, max_length=64, )
  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)
  return peft_model_text_output

for i in range(0,5):
  text=test['Text'][i]
  print(example(text))

"""**rouge，bertscore，meteor，bleu**"""

!pip install bert-score
!pip install sacrebleu

from evaluate import evaluator
from transformers import AutoModelForSeq2SeqLM, pipeline

peft_model_summaries = []
for text in test['Text']:
  input_prompt= f"Summarize:{text}"
  input_ids = tokenizer(input_prompt, padding="max_length", truncation=True, max_length=512, return_tensors="pt").input_ids.to(device)
  peft_model_outputs = lora_model.generate(input_ids=input_ids, max_length=64)
  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)
  peft_model_summaries.append(peft_model_text_output)

human_baseline_summaries=test['Summary']
zipped_summaries = list(zip(human_baseline_summaries, peft_model_summaries))
df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'peft_model_summaries'])

#rouge score
from evaluate import load
rouge = load('rouge')
rouge_score = rouge.compute(
    predictions=peft_model_summaries,
    references=human_baseline_summaries[0:len(peft_model_summaries)],
    use_aggregator=True,
    use_stemmer=True,
)
print(rouge_score)

rouge_score

#bertscore
from bert_score import BERTScorer
scorer = BERTScorer(model_type='bert-base-uncased')
P, R, F1 = scorer.score(human_baseline_summaries, peft_model_summaries)
print(f"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}")

f"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}"

#bleu
from sacrebleu import corpus_bleu
bleu_score = corpus_bleu(human_baseline_summaries, peft_model_summaries)
print("BLEU Score:", bleu_score.score)

"BLEU Score:", bleu_score.score

from nltk.translate import meteor_score
import nltk
nltk.download('wordnet')

reference_tokens = [summary.split() for summary in human_baseline_summaries]
generated_tokens = [summary.split() for summary in peft_model_summaries]

meteor_scores = [meteor_score.single_meteor_score(ref, gen) for ref, gen in zip(reference_tokens, generated_tokens)]
average_meteor_score_value = sum(meteor_scores) / len(meteor_scores)
print("Average METEOR Score:", average_meteor_score_value)

"Average METEOR Score:", average_meteor_score_value