# -*- coding: utf-8 -*-
"""bart_100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14NTxktlTVyQv-z6PQFQ4uEN5TO5E1hA8

**Dependencies**
"""

!pip install transformers==3.1.0
!pip install datasets evaluate transformers[sentencepiece]
!pip install accelerate
!pip install numpy
#!pip install cloud-tpu-client==0.10 torch==1.11.0
!apt install git-lfs
!pip install datasets evaluate transformers rouge-score nltk
!pip install -q peft transformers datasets
!pip install --upgrade tensorflow==2.15.*
!pip install --upgrade datasets pyarrow
!pip install -q bitsandbytes datasets accelerate loralib
!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git
!pip install tensorboard tensorboardX
!pip install -U tensorboard-plugin-profile

# checking the GPU and CUDA
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

# checking the RAM
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb < 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')

# Commented out IPython magic to ensure Python compatibility.
# download file from drive，and only use it when you need lora from scratch.py.
from google.colab import drive
drive.mount('/content/drive')
# %cd "/content/drive/My Drive/Colab"
# %ls

import transformers
print(transformers.__version__)

from transformers.utils import send_example_telemetry
send_example_telemetry("summarization_notebook", framework="pytorch")

#parallel computing
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# storing the authentication token from the Hugging Face.
from huggingface_hub import notebook_login
notebook_login()

# base model
model_checkpoint="facebook/bart-base"

"""**Load Dataset-amazon review**"""

# import the datasets from huggingface
from os.path import split
from datasets import load_dataset

raw_datasets = load_dataset("jhan21/amazon-food-reviews-dataset")
raw_datasets=raw_datasets['train'] # the original dataset only has one key “train".

from sklearn.model_selection import train_test_split
from datasets import Dataset

x = raw_datasets['Text']
y = raw_datasets['Summary']

raw_datasets = Dataset.from_dict({"Text": x, "Summary": y})

# remove the none data
index_to_remove = [i for i, (text, summary) in enumerate(zip(x, y)) if not text or not summary]

filtered_x = [text for i, text in enumerate(x) if i not in index_to_remove]
filtered_y = [summary for i, summary in enumerate(y) if i not in index_to_remove]

# split the dataset
x_train, x_temp, y_train, y_temp = train_test_split(filtered_x, filtered_y, test_size=0.2, random_state=42)
x_valid, x_test, y_valid, y_test = train_test_split(x_temp, y_temp, test_size=0.1, random_state=42)

def dict(x,y):
  set={'Text': x, 'Summary': y}
  set=Dataset.from_dict(set)
  return set


train1 = dict(x_train,y_train)
validation1 =dict(x_valid,y_valid)
test1 = dict(x_test, y_test)

# training the model on  a small subset
import pandas as pd

def split(set,ratio):
  cut_point=int(len(set)*ratio)
  small_set=Dataset.from_dict(set[0:cut_point])
  return small_set

ratio=1
train = split(train1,ratio)
validation= split(validation1,ratio)
test= split(test1,0.05)

print(train)
print(validation)
print(test)

"""**Dataset Preprocessing**

Initiate Tokenizer for BERT
"""

#  instantiating the tokenizer
import nltk
nltk.download('punkt')

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,device_map="auto")

"""Preprocess"""

# prompt
def start_prompt(num,text):
  if num==1:
    prompt=f'Summarize the following text with a maximum of {len(tokenizer.tokenize(text))} words:'
  if num==2:
    sentences = [sentence.strip() for sentence in text.split(".") if sentence.strip()]
    prompt=f'Can you succinctly summarize in {len(sentences)} sentences the following passage?'
  if num==3:
    prompt='Could you provide me with a brief summary of the following text?'
  if num==4:
    prompt='summarize:'
  if num==5:
    prompt='TL;DR:'
  return prompt

#  token legnth
import random


def preprocess_function(example):
  try:
        prompt = [start_prompt(random.randint(1, 5), sum) + doc for doc, sum in zip(example["Text"], example["Summary"])]
        example['input_ids'] = tokenizer(prompt, padding=True, truncation=True,  max_length=512, return_tensors="pt").input_ids.to(device)
        example['labels'] = tokenizer(example["Summary"], padding=True, truncation=True, max_length=64, return_tensors="pt").input_ids.to(device)
        return example
  except Exception as e:
        print("An error occurred while processing the following example:")
        print(example)
        print("Error message:", e)
        return None

preprocess_function(train[1])

# Apply the preprocessing function to the dataset
try:
  tokenized_train = train.map(preprocess_function, batched=True)
except Exception as e:
    print("An error occurred during mapping:")
    print("Error message:", e)
tokenized_validation = validation.map(preprocess_function, batched=True)

print(f"Shapes of the datasets:")
print(f"Training: {tokenized_train.shape}")
print(f"Validation: {tokenized_validation.shape}")

"""**Fine-Tune BERT with PEFT LoRA**"""

# load BERT model from the hub
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer

base_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

base_model.to(device)

base_model.device

def trainable_model_params(model):
  trainable_params=0
  all_params=0
  for _,param in model.named_parameters():
    all_params+=param.numel()
    if param.requires_grad:
      trainable_params+=param.numel()
  return f"trainable params: {trainable_params}\nall params: {all_params}\npercentage of trainable params: {100*trainable_params/all_params}"
print(trainable_model_params(base_model))

"""Setup the LoRA model for Fine-Tuning"""

# set up the configuration
from peft import LoraConfig, get_peft_model, TaskType

# Define LoRA Config
lora_config = LoraConfig(
    r=4, # Rank
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    fan_in_fan_out=False,
    task_type="SEQ_2_SEQ_LM",
)

# add LoRA adaptor layers/parameters to the original LLM to be trained.
model = get_peft_model(base_model,lora_config)
print(trainable_model_params(model))

"""Define the hyperparameters for training"""

from transformers import GenerationConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer,DataCollatorForSeq2Seq
import time

output_dir = f'bart-lora-ar'

# Define training args
training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    auto_find_batch_size=True,
    num_train_epochs=1,
    logging_strategy="steps",
    logging_steps=500,
    gradient_accumulation_steps=2,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=500,
    load_best_model_at_end=True,
    weight_decay=0.01,
    push_to_hub=True,
    fp16=True,
    predict_with_generate=True,
    report_to=["tensorboard"],
    resume_from_checkpoint=True,
)

# data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# create a trainer
peft_trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_validation,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

"""Train"""

# Commented out IPython magic to ensure Python compatibility.
# use TensorBoard to monitor the training process
# %tensorboard --logdir '{output_dir}'/runs

# train model
peft_trainer.train()

peft_trainer.evaluate()

# Save our LoRA model & tokenizer results in local drive.
peft_model_path="./bart-lora-ar"
peft_trainer.model.save_pretrained(peft_model_path)
#peft_trainer.model.push_to_hub('t5-lora-ar')
tokenizer.save_pretrained(peft_model_path)

from peft import PeftModel, PeftConfig
peft_model = PeftModel.from_pretrained(model,
                                       peft_model_path,
                                       torch_dtype=torch.bfloat16,
                                       is_trainable=False)

import shutil

# 模型目录在 Colab 中的路径
colab_model_dir = '/content/bart-lora-ar'
# 模型目录要保存到 Google Drive 中的路径
drive_model_dir = '/content/drive/My Drive/bart-lora-ar-final'

# 将整个目录复制到 Google Drive
shutil.copytree(colab_model_dir, drive_model_dir)

print("Model directory copied to Google Drive.")

# put the model to the CUDA
peft_model.to(device)

peft_trainer.get_learning_rates()

peft_trainer.get_train_dataloader()

"""**Evaluate the Model with ROUGE**"""

from transformers import AutoModelForSeq2SeqLM

# 加载模型
new_model = AutoModelForSeq2SeqLM.from_pretrained("./bart-lora-ar")
new_model.to(device)

from transformers import AutoModelForSeq2SeqLM

# 加载模型
new_model = AutoModelForSeq2SeqLM.from_pretrained('/content/drive/MyDrive/bart-lora-ar-final')
new_model.to(device)

#basic inference
from evaluate import evaluator
from transformers import AutoModelForSeq2SeqLM, pipeline
def example(text):
  print(text)
  input_prompt= f"Summarize:{text}"
  input_ids = tokenizer(input_prompt, padding="max_length", truncation=True, max_length=512, return_tensors="pt").input_ids.to(device)
  peft_model_outputs = new_model.generate(input_ids=input_ids, max_length=64, )
  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)
  return peft_model_text_output

for i in range(0,5):
  text=test['Text'][i]
  print(example(text))

!pip install bert-score
!pip install sacrebleu

from evaluate import evaluator
from transformers import AutoModelForSeq2SeqLM, pipeline

peft_model_summaries = []
for text in test['Text']:
  input_prompt= f"Summarize:{text}"
  input_ids = tokenizer(input_prompt, padding="max_length", truncation=True, max_length=512, return_tensors="pt").input_ids.to(device)
  peft_model_outputs = new_model.generate(input_ids=input_ids, max_length=64)
  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)
  peft_model_summaries.append(peft_model_text_output)

human_baseline_summaries=test['Summary']
zipped_summaries = list(zip(human_baseline_summaries, peft_model_summaries))
df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'peft_model_summaries'])

#rouge score
from evaluate import load
rouge = load('rouge')
rouge_score = rouge.compute(
    predictions=peft_model_summaries,
    references=human_baseline_summaries[0:len(peft_model_summaries)],
    use_aggregator=True,
    use_stemmer=True,
)
print(rouge_score)

#bertscore
from bert_score import BERTScorer
scorer = BERTScorer(model_type='bert-base-uncased')
P, R, F1 = scorer.score(human_baseline_summaries, peft_model_summaries)
print(f"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}")

#bleu
from sacrebleu import corpus_bleu
bleu_score = corpus_bleu(human_baseline_summaries, peft_model_summaries)
print("BLEU Score:", bleu_score.score)

from nltk.translate import meteor_score
import nltk
nltk.download('wordnet')

reference_tokens = [summary.split() for summary in human_baseline_summaries]
generated_tokens = [summary.split() for summary in peft_model_summaries]

meteor_scores = [meteor_score.single_meteor_score(ref, gen) for ref, gen in zip(reference_tokens, generated_tokens)]
average_meteor_score_value = sum(meteor_scores) / len(meteor_scores)
print("Average METEOR Score:", average_meteor_score_value)